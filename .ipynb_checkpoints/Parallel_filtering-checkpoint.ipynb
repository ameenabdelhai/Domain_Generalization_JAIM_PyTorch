{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# config and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'BaseNet_clstm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_recall_fscore_support\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBaseNet_clstm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MMDNet, PredictNet\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'BaseNet_clstm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from BaseNet_clstm import *\n",
    "from model import MMDNet, PredictNet\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'BaseNet_clstm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_recall_fscore_support\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBaseNet_clstm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MMDNet, PredictNet\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'BaseNet_clstm'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================== Hyper-parameters =====================================\n",
    "batch_size = 512\n",
    "lr = 0.001\n",
    "l2_decay = 5e-4\n",
    "num_epochs = 14\n",
    "input_size = 150\n",
    "hidden_size = 128 # output of hidden features\n",
    "sequence_length = 100 # number of visits\n",
    "num_layers = 2\n",
    "num_classes = 1 # dense 1 for binary classification\n",
    "# ===================================== LOAD DATA =====================================\n",
    "\n",
    "print(\"loading data...\")\n",
    "path = r\"/home/ameen/Desktop/JAIM/data/ML\"\n",
    "X_Temple = np.load(path + \"Temple_X.csv\")\n",
    "y_Temple = np.load(path + \"Temple_y.csv\")\n",
    "print(\"Temple data:\", X_Temple.shape)\n",
    "\n",
    "X_PSU = np.load(path + \"PSU_X.csv\")\n",
    "y_PSU = np.load(path + \"PSU_y.csv\")\n",
    "print(\"PSU data:\", X_PSU.shape)\n",
    "\n",
    "X_Hopkins = np.load(path + \"Hopkins_X.csv\")\n",
    "y_Hopkins = np.load(path + \"Hopkins_y.csv\")\n",
    "print(\"Hopkins data:\", X_Hopkins.shape)\n",
    "\n",
    "X_Geisinger = np.load(path + \"Geisinger_X.csv\")\n",
    "y_Geisinger = np.load(path + \"Geisinger_y.csv\")\n",
    "print(\"Geisinger data:\", X_Geisinger.shape)\n",
    "\n",
    "X_PITT = np.load(path + \"PITT_X.csv\")\n",
    "y_PITT = np.load(path + \"PITT_y.csv\")\n",
    "print(\"PITT data:\", X_PITT.shape)\n",
    "\n",
    "# --------------------------------\n",
    "\n",
    "# reshape to 3D\n",
    "print(\"\\nReshaping to 3D...\")\n",
    "X_Temple = X_Temple.reshape((X_Temple.shape[0], 100, 150))\n",
    "print(\"Temple - 3D tensor:\", X_Temple.shape)\n",
    "\n",
    "X_PSU = X_PSU.reshape((X_PSU.shape[0], 100, 150))\n",
    "print(\"PSU - 3D tensor:\", X_PSU.shape)\n",
    "\n",
    "X_Hopkins = X_Hopkins.reshape((X_Hopkins.shape[0], 100, 150))\n",
    "print(\"Hopkins - 3D tensor:\", X_Hopkins.shape)\n",
    "\n",
    "X_PITT = X_PITT.reshape((X_PITT.shape[0], 100, 150))\n",
    "print(\"PITT - 3D tensor:\", X_PITT.shape)\n",
    "\n",
    "X_Geisinger = X_Geisinger.reshape((X_Geisinger.shape[0], 100, 150))\n",
    "print(\"Geisinger - 3D tensor:\", X_Geisinger.shape)\n",
    "\n",
    "# -----------------------------------------------\n",
    "print(\"preparing for DataLoader (X, y) tuple...\")\n",
    "Temple_data = []\n",
    "for i in range(len(X_Temple)):\n",
    "    t_ = (X_Temple[i], y_Temple[i])\n",
    "    Temple_data.append( t_ )\n",
    "\n",
    "PSU_data = []\n",
    "for i in range(len(X_PSU)):\n",
    "    t_ = (X_PSU[i], y_PSU[i])\n",
    "    PSU_data.append( t_ )\n",
    "\n",
    "Geisinger_data = []\n",
    "for i in range(len(X_Geisinger)):\n",
    "    t_ = (X_Geisinger[i], y_Geisinger[i])\n",
    "    Geisinger_data.append( t_ )\n",
    "\n",
    "Hopkins_data = []\n",
    "for i in range(len(X_Hopkins)):\n",
    "    t_ = (X_Hopkins[i], y_Hopkins[i])\n",
    "    Hopkins_data.append( t_ )\n",
    "\n",
    "PITT_data = []\n",
    "for i in range(len(X_PITT)):\n",
    "    t_ = (X_PITT[i], y_PITT[i])\n",
    "    PITT_data.append( t_ )\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "# split train validation -> 5% validation from training sets\n",
    "X_Temple_train, X_Temple_val = train_test_split(\n",
    "    Temple_data, test_size=0.05, random_state=42)\n",
    "\n",
    "X_Hopkins_train, X_Hopkins_val = train_test_split(\n",
    "    Hopkins_data, test_size=0.05, random_state=42)\n",
    "\n",
    "X_PITT_train, X_PITT_val = train_test_split(\n",
    "    PITT_data, test_size=0.05, random_state=42)\n",
    "\n",
    "X_PSU_train, X_PSU_val = train_test_split(\n",
    "    PSU_data, test_size=0.05, random_state=42)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"X_Temple_train %d \\n\"\n",
    "    \"X_Temple_val %d \\n\"\n",
    "    \"X_PSU_train %d \\n\"\n",
    "    \"X_PSU_val %d \\n\"\n",
    "    \"X_Hopkins_train %d \\n\"\n",
    "    \"X_Hopkins_val %d \\n\"\n",
    "    \"X_PITT_train %d \\n\"\n",
    "    \"X_PITT_val %d \\n\"\n",
    "    \"X_Geisinger -> test %d \\n\"\n",
    "\n",
    "    % (\n",
    "        len(X_Temple_train), len(X_Temple_val),\n",
    "        len(X_PSU_train), len(X_PSU_val),\n",
    "        len(X_Hopkins_train), len(X_Hopkins_val),\n",
    "        len(X_PITT_train), len(X_PITT_val),\n",
    "        len(Geisinger_data),\n",
    "    )\n",
    ")\n",
    "\n",
    "# DataLoader\n",
    "print(\"DataLoader...\")\n",
    "# test\n",
    "tgt_test_dataloader = DataLoader(dataset=Geisinger_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# train\n",
    "src_1_train_dataloader = DataLoader(dataset=X_Temple_train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "src_1_validate_dataloader = DataLoader(dataset=X_Temple_val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "src_2_train_dataloader = DataLoader(dataset=X_Hopkins_train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "src_2_validate_dataloader = DataLoader(dataset=X_Hopkins_val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "src_3_train_dataloader = DataLoader(dataset=X_PSU_train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "src_3_validate_dataloader = DataLoader(dataset=X_PSU_val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "src_4_train_dataloader = DataLoader(dataset=X_PITT_train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "src_4_validate_dataloader = DataLoader(dataset=X_PITT_val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# ==========================================================================================\n",
    "device = torch.device(\"cpu\")\n",
    "len_tgt = len(tgt_test_dataloader)\n",
    "len_src_1_train = len(src_1_train_dataloader)\n",
    "len_src_2_train = len(src_2_train_dataloader)\n",
    "len_src_3_train = len(src_3_train_dataloader)\n",
    "len_src_4_train = len(src_4_train_dataloader)\n",
    "\n",
    "\n",
    "src_loss_list = []\n",
    "total_loss_list = []\n",
    "tgt_val_loss_list = []\n",
    "\n",
    "seed = 32\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Initialize network\n",
    "BaseNet = BaseNet_clstm(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "TransferNet = MMDNet().to(device)\n",
    "TaskNet = PredictNet().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "task_criterion = nn.BCELoss()\n",
    "\n",
    "# optimizer = optim.Adam(BaseNet.parameters(), lr=lr)\n",
    "optimizer = optim.Adam([\n",
    "    {'params': BaseNet.parameters()},\n",
    "    {'params': TransferNet.parameters()},\n",
    "    {'params': TaskNet.parameters()}], lr=lr, weight_decay=l2_decay)\n",
    "\n",
    "\n",
    "src_loss_list = []\n",
    "total_loss_list = []\n",
    "tgt_val_loss_list = []\n",
    "\n",
    "best_bce = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
